# LLM Configuration
# Your Google API key for Gemini Pro access
GOOGLE_API_KEY=your_google_api_key_here

# Optional: OpenAI API Key (if needed as fallback)
# Your OpenAI API key if using OpenAI models as fallback
OPENAI_API_KEY=your_openai_api_key_here

# Logging Configuration
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
# Log format string
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# Evaluation Settings
# Directory for evaluation results
EVAL_RESULTS_DIR=eval_results
# Directory for evaluation data
EVAL_DATA_DIR=data

# Model Settings
# Temperature for response generation (0.0 to 1.0)
MODEL_TEMPERATURE=0.7
# Top-p sampling parameter (0.0 to 1.0)
MODEL_TOP_P=0.8
# Top-k sampling parameter
MODEL_TOP_K=40
# Maximum tokens in model output
MAX_OUTPUT_TOKENS=2048 